# 07.030 ASCII and Unicode

There are two common standards for representing data in a computer.

Computers only work on bit patterns.  The computer is designed to treat those bit patterns as integers.  

Character data is not "natural" for computers.  The bit pattern 00000011 may can be interpreted as an integer 3.  But there is no real reason to think that the bit pattern 01000011 would represent the character 'C'.  What we need is a standard.

In the 1950s there were several competing standards for how to represent computer data.  Generally, ASCII won.
## ASCII  Letters in 7 (or 8) bits

ASCII stands for "American Standard Code for Information Interchange."  The word "American" is significant.  Most of the development of modern computers after WWII happened in the United States.  The concern was representing English.

Teletype machines were commonly attached to computers as terminals.  Teletype machines were also used to send telegrams which were an important form of communication.  Teletype machines were set up to communicate with 7 bits.  $7^2 - 128$.  So the numbers 0 through 127 may be represented.  Or there may be 127 characters.  In English we need the following at a minimum:

&nbsp;|&nbsp;
---:|:---:
Upper Case Letters|26
Lower Case Letters|26
Digits|10
Basic Punctuation:|10
TOTAL:|72

So 72 fits easily within 128.  In addition, Teletype machines themselves required 32 different values for control signals.  But this still brings us to 104.  That leaves roughly 20 combinations for things like curly braces and dollar signs.

# Bytes

A byte is a group of bits.  Computer and computer communications group bits into bytes.  *On modern computers, a byte is a group of 8 bits.*  This has been true of most computers built after about 1955.

### ASCII codes from [https://www.asciitable.com/](https://www.asciitable.com/)

![https://www.asciitable.com/](asciifull.gif)

The ASCII codes are great for displaying things in English, especially American English.  It includes the $ and &cent; signs.  

However, if you go to NATO countries of western Europe, it starts breaking down.

Teletypes used 7 bits, but by the late 1950s we had moved beyond teletypes.  The new output devices and computers had 8 bits per byte.

Adding 1 more bit allows for another 128 characters.  So "Extended ASCII" was developed.  It included the characters needed for most western European languages as well as some useful symbols such as &pi;.

![https://www.asciitable.com/](extend.gif)

## Unicode

But Extended ASCII was still limited to modern western European languages.  Something more was needed.

Original Unicode used 16 bits.  Sixteen bits allows 256 different alphabets, each with 256 characters.  Later versions of unicode allow up to 32 bits.  

### [Link to Unicode Character Charts](https://unicode.org/charts/)

The ASCII codes were absorbed into the Unicode system.  The first alphabet is ASCII.  if the first 8 bits of a Unicode character is all zeros, it is effectively an ASCII code.

Unicode character are usually represented as hexadecimal values (Base 16)  16-bit unicode requires 4 hexadecimal integers.

## Java and Unicode

Java uses Unicode to represent characters.  A unicode character may be represented by putting \u in front of the unicode hex value.  For example, "C" would be "\u0043"

```java
        System.out.println("\u0043");
        System.out.println("\u0411");
        System.out.println("Ð‘");
```
produces
```text
        C
        Ð‘
        Ð‘
```

### Extended Unicode

Unicode has been extended to include pictographic language.  It has also included common emoji. 

[https://unicode.org/emoji/charts/full-emoji-list.html](https://unicode.org/emoji/charts/full-emoji-list.html)

```java
        System.out.println("\uD83D\uDCA9");
```
produces
```text
ðŸ’©
```

## Fonts vs Unicode

Just because there is a Unicode character does not mean a certain character will show up on your computer.  To display a given character you computer must have a font installed that includes that character.  

One example is flags.  All countries now have their flags as unicode characters.  Here is the Ukrainian flag.  ðŸ‡ºðŸ‡¦

If you are on Linux or a Mac you should see the blue-over-yellow flag.  But if you are on a Windows computer, it probably just shows an empty box because at least up through Windows 10 the flag font was not installed (I am not sure if it is in Windows 11).